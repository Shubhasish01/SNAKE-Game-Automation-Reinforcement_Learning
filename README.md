# ğŸ Snake Game â€“ Automation via Reinforcement Learning

This project implements the classic Snake Game and trains an agent to play it using **Deep Q-Learning** (a type of Reinforcement Learning). The agent learns to improve over time by interacting with the game environment and maximizing its score using feedback in the form of rewards.


## ğŸš€ Project Highlights

- ğŸ® Snake Game implemented using **Pygame**
- ğŸ§  Agent powered by **Deep Q-Network (DQN)**
- ğŸ“ˆ Learns through **Experience Replay** and **Short/Long-Term Memory**
- ğŸ“¦ Modular code split into environment, agent, model, and helper files


## ğŸ§± Project Structure

ğŸ“ SNAKE-Game-Automation-Reinforcement_Learning

SNAKE-Game-Automation-Reinforcement_Learning/

â”œâ”€â”€ agent.py # Reinforcement Learning agent logic

â”œâ”€â”€ game.py # Snake game environment

â”œâ”€â”€ helper.py # Utility functions

â”œâ”€â”€ model.py # Neural Network architecture

â”œâ”€â”€ snake_game_human.py # Play Snake manually (for testing)

â”œâ”€â”€ arial.ttf # Font for game rendering

â”œâ”€â”€ model/ # Trained models (saved checkpoints)

â”œâ”€â”€ pycache/ # Ignored by Git (bytecode)

â””â”€â”€ README.md # You are here


## ğŸ§  Learning Algorithm

The agent is trained using **Deep Q-Learning** with the following key ideas:

- **State Representation**: Position of snake, direction, food, and potential dangers
- **Action Space**: [straight, right turn, left turn]
- **Reward Function**:
  - `+10` for eating food  
  - `-10` for hitting a wall or self  
  - `0` otherwise
- **Experience Replay**: Uses past experiences to break correlation and improve stability
- **Epsilon-Greedy Strategy**: Balances exploration and exploitation


## ğŸ› ï¸ Installation & Setup

1. Clone the repository:
   git clone https://github.com/Shubhasish01/SNAKE-Game-Automation-Reinforcement_Learning.git
   cd SNAKE-Game-Automation-Reinforcement_Learning
   
Install dependencies:
pip install -r requirements.txt

Train the agent:
python agent.py

Play manually:
python snake_game_human.py

ğŸ“Œ TODOs & Future Work
 Add support for saving/loading models

 Visualize Q-values in real-time

 Experiment with other RL algorithms (e.g., PPO, A3C)

ğŸ¤ Contributions
Feel free to fork the repo, improve the code, and submit a pull request!

ğŸ“œ License
MIT License. See LICENSE file for details.

ğŸ™‹â€â™‚ï¸ Author
Shubhasish
GitHub Profile
